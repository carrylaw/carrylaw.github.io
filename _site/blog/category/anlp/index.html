<!DOCTYPE html>
<html lang="en-us">

	<head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <title>
    
      Anlp &middot; 数分学长
    
  </title>

  <!-- CSS -->
  <link rel="stylesheet" href="/public/css/poole.css">
  <link rel="stylesheet" href="/public/css/syntax.css">
  <link rel="stylesheet" href="/public/css/lanyon.css">
  <link rel="stylesheet" href="/public/css/minddust.css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=PT+Serif:400,400italic,700%7CPT+Sans:400">

  <!-- Icons -->
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/public/apple-touch-icon-precomposed.png">
  <link rel="shortcut icon" href="/public/favicon.ico">

  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="RSS" href="/atom.xml">
	
	<!--typeahead-->
  <link rel="stylesheet" href="/public/css/jquery.typeahead.css">
  <link rel="stylesheet" href="/public/css/style.css">
  <script src="https://cdn.bootcss.com/jquery/2.1.0/jquery.min.js"></script>
  <script src="https://cdn.bootcss.com/waypoints/4.0.1/jquery.waypoints.min.js"></script>
  <script src="/public/js/jquery.typeahead.js"></script>
  <script src="/public/js/tools.js"></script>

</head>


	<body>

		<!-- Target for toggling the sidebar `.sidebar-checkbox` is for regular
     styles, `#sidebar-checkbox` for behavior. -->
<input type="checkbox" class="sidebar-checkbox" id="sidebar-checkbox">

<!-- Toggleable sidebar -->
<div class="sidebar" id="sidebar">
  <div class="sidebar-item">
    <p><font size="5" color="darkgray" face="times new roman">Carry.law</font></p>
	<p><font size="2" color="darkgray" face="微软雅黑">一枚最不像数分师的数据分析师</font></p>
  </div>

  <nav class="sidebar-nav">
    <a class="sidebar-nav-item" href="/">主页</a>

    

    
    
      
        
      
    
      
        
      
    
      
        
          <a class="sidebar-nav-item" href="/about/">关于</a>
        
      
    
      
    
      
    
      
        
      
    
    <a class="sidebar-nav-item" id="articles" expanded='true'>博文馆</a>
   	
      
		<a class="sidebar-nav-item category-name-show" name="category-name" href="/blog/category/anlp/">- 自然语言处理</a>
    
		<a class="sidebar-nav-item category-name-show" name="category-name" href="/blog/category/bmarchine/">- 机器学习实战</a>
    
		<a class="sidebar-nav-item category-name-show" name="category-name" href="/blog/category/cpython/">- Python技术</a>
    
		<a class="sidebar-nav-item category-name-show" name="category-name" href="/blog/category/emysql/">- 数据库技术</a>
    
		<a class="sidebar-nav-item category-name-show" name="category-name" href="/blog/category/ynews/">- 相关资讯</a>
    
    <span class="sidebar-nav-item">&copy; 2017. All rights reserved.</span>
  </nav>
</div>


		<!-- Wrap is the content to shift when toggling the sidebar. We wrap the
         content to avoid any CSS collisions with our real content. -->
		<div class="wrap">
			<div class="masthead">
				<div class="container">
					<img src="/public/logo.png" class="masthead-img masthead-inner masthead-shrink"/>						
					<h3 class="masthead-title masthead-inner masthead-shrink" >
					
						<a href="/blog/category/anlp/" title="Home">自然语言处理</a>
			            <small class="font-kt">分类文章</small>					    
    				
					</h3>
					<div class="masthead-nav masthead-inner">
						<nav class="nav-top">
	<ul class="text-xs-center">
		<li>
			<!--检索框-->
			<form id="search-bar" action="/search" target="_blank" onsubmit="encodeKeyword()">
	<div class="typeahead__container">
		<div class="typeahead__field">

			<span class="typeahead__query">
                <input class="js-typeahead"
                       name="q"
                       type="search"
                       autocomplete="off">
            </span>
			<span class="typeahead__button" hidden="hidden">
                <button type="submit">
                    <span class="typeahead__search-icon"></span>
				</button>
			</span>

		</div>
	</div>
</form>    
		</li>
		<li><a href="#" id="search-btn" class="label label-darkblue tooltip" title="search"><span class="icon icon-search"></span></a></li>
		<li class="site-name"><a href="/" class="label label-darkblue character">数分学长</a></li>
		<li><a href="/atom.xml" class="label label-darkblue tooltip" title="RSS" target="_blank"><span class="icon icon-rss"></span></a></li>
		<li><a href="https://github.com/carrylaw" class="label label-darkblue tooltip" title="GitHub" target="_blank"><span class="icon icon-github"></span></a></li>
	</ul>
</nav>

<!--		<li><a href="http://www.jianshu.com/u/5caa2bf26f9b" class="label label-darkblue tooltip" title="jianshu" target="_blank"><span class="icon icon-jian"></span></a></li>-->
					</div>
				</div>
			</div>

			<div class="container content">
				<div class="posts">

 
 	  <div class="post hidden">
    <h1 class="post-title">
      <a href="/anlp/2017/08/19/nlp02/">
        文本分类器构建方法及程序设计
      </a>
    </h1>
    <span class="post-date" >19 Aug 2017</span>
    <div class="post-content">
         
            自然语言处理（NLP）很基础的一块便是构建文本分类器，其应用领域也比较广泛，例如新闻题材、垃圾邮件的自动分类等。现阶段构建文本分类器的算法大致包括：朴素贝叶斯、SVM支持向量机、最大熵分类器和BP神经网络等。本文选取SVM算法对某银行客服录音数据进行分类，分类方法及程序设计如下：

注：在构建文本分类器之前，请先确定配备以下环境    
Python 3.5及以上版本   
jieba分词包
详情请见: python之结巴中文分词

· 数据预处理方法论

1.导入原始语料（存储格式：TXT，utf-8）   
 原始语料下载地址 
&lt;div align="center"&gt;

&lt;/div&gt;    
2.样本整理
&lt;div align="center"&gt;

&lt;/div&gt;   
3.导入停用词库和自定义词库    
 停用词库和自定义词库下载地址
&lt;div align="center"&gt;

&lt;/div&gt;       
4.文本分词：   
- 删除第1行和最后2行语句（规范话术）     
- 去除停用词   
- 添加自定义词典   
- 结巴分词（精确模式）   
 清洗目的：   
 1）避免无意义词汇的出现，干扰后续TF-IDF词频提取的准确度    
 2）降低SVM支持向量机分类计算时的空间复杂度 
&lt;div align="center"&gt;

&lt;/div&gt;    

· 数据预处理程序设计

import jieba
import jieba.analyse

#添加自定义词典
jieba.load_userdict('e:\\lexicon\\CBwords.txt')

# 创建停用词列表函数
def creadstoplist(stopwordspath):
    stwlist = [line.strip() for line in open(stopwordspath, 'r', encoding='utf-8').readlines()]
    return stwlist

# 对句子进行分词
def seg_sentence(sentence):
    wordList = jieba.cut(sentence.strip())
    stwlist = creadstoplist('e:\\lexicon\\CBstopwords.txt') #加载停用词路径
    outstr = ''
    for word in wordList:
        if word not in stwlist:
                if word != '\t':
                    outstr += word
                    outstr += " "
    return outstr

#删除第1行和最后3行语句
infile = open('e:\\SVM预处理输入\\in1.txt', 'r', encoding='utf-8')
line_new = infile.readlines() #注意readline 和readlines
infile_new = ''.join(line_new[1:-3]) 
outfile_new = open("e:\\new.txt","w",encoding='utf-8')
outfile_new.write(infile_new)
infile.close()
outfile_new.close()

#结果输出到txt文件夹中
infile_new1 = open("e:\\new.txt","r",encoding='utf-8')
outfile = open('e:\\SVM预处理输出\\outfile1.txt', 'w', encoding='utf-8')
for line in infile_new1:
    line_seg = seg_sentence(line)
    outfile.write(line_seg+'\n')
infile_new1.close()
outfile.close()

import jieba
import jieba.analyse

#添加自定义词典
jieba.load_userdict('e:\\lexicon\\CBwords.txt')

# 创建停用词列表函数
def creadstoplist(stopwordspath):
    stwlist = [line.strip() for line in open(stopwordspath, 'r', encoding='utf-8').readlines()]
    return stwlist

# 对句子进行分词
def seg_sentence(sentence):
    wordList = jieba.cut(sentence.strip())
    stwlist = creadstoplist('e:\\lexicon\\CBstopwords.txt') #加载停用词路径
    outstr = ''
    for word in wordList:
        if word not in stwlist:
                if word != '\t':
                    outstr += word
                    outstr += " "
    return outstr

#删除第1行和最后3行语句
infile = open('e:\\SVM预处理输入\\in1.txt', 'r', encoding='utf-8')
line_new = infile.readlines() #注意readline 和readlines
infile_new = ''.join(line_new[1:-3]) 
outfile_new = open("e:\\new.txt","w",encoding='utf-8')
outfile_new.write(infile_new)
infile.close()
outfile_new.close()

#结果输出到txt文件夹中
infile_new1 = open("e:\\new.txt","r",encoding='utf-8')
outfile = open('e:\\SVM预处理输出\\outfile1.txt', 'w', encoding='utf-8')
for line in infile_new1:
    line_seg = seg_sentence(line)
    outfile.write(line_seg+'\n')
infile_new1.close()
outfile.close()


· SVM自动分类器方法论

1.导入数据集（存储格式：CSV）    
 预处理后数据集下载地址
&lt;div align="center"&gt;

&lt;/div&gt;   
 Num表示id号（随意编排）    
 Content表示数据预处理后文本内容    
 Lable表示文本内容所对应的分类标签    
2.划分为训练集和测试集A   
3.转化为词频向量化矩阵，并计算TF-IDF值   
4.训练SVM文本分类器   
5.测试分类结果   
6.计算精确度   
7.稳健性检验   
 检验目的：提高文本分类器可信度   
 检验方法分别为以下两种：   
 1）训练集不变，将测试集A替换成测试集B，由此预测测试集B的分类结果     
 2）训练集与测试集A进行同类别相互替换，由此重新训练并预测分类器结果     
8.整理预测结果
&lt;div align="center"&gt;

&lt;/div&gt;  
 label 人工分类标签   
 predict 分类器预测标签

· SVM自动分类器程序设计

import csv
import numpy as np
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.svm import SVC
from sklearn.pipeline import Pipeline
from sklearn import metrics
from sklearn.grid_search import GridSearchCV

# 1.读取训练集
def readtrain():
    with open('e:\\test1.csv', 'r') as csvfile: 
        reader = csv.reader(csvfile)
        column1 = [row for row in reader]
    content_train = [i[1] for i in column1[1:]] #第一列为文本内容，并去除列名
    opinion_train = [i[2] for i in column1[1:]] #第二列为分类标签，并去除列名
    print ('数据集共有 %s 条句子' % len(content_train)) 
    train = [content_train, opinion_train]
    return train

def changeListCode(b):
    a = []
    for i in b:
        a.append(i.decode('utf8'))
    return a
train = readtrain() #读取readtrain数据集
content = train[0] #第一行文本内容赋给content
opinion = train[1] #第二行分类标签赋给opinion

# 2.划分训练集与测试集
train_content = content[:85]  #训练集文本内容
test_content = content[85:]   #测试集文本内容
train_opinion = opinion[:85]  #训练集分类标签
test_opinion = opinion[85:]   #测试集分类标签（用于计算精确度）

# 3.文本向量化
vectorizer = CountVectorizer() #将向量化函数赋给vectorizer
tfidftransformer = TfidfTransformer()
tfidf = tfidftransformer.fit_transform(vectorizer.fit_transform(train_content))  # 先转换成词频矩阵，再计算TFIDF值
print (tfidf.shape)

# 4.训练文本分类器
text_clf = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), ('clf', SVC(C=0.99, kernel = 'linear'))])
text_clf = text_clf.fit(train_content, train_opinion)

# 5.预测文本分类结果
predicted = text_clf.predict(test_content)
print ("打印预测结果：")
print (predicted) #打印预测分类结果

# 6.计算精确度
print ("============================================")
print ("计算预测结果精确度")
print ('SVC',np.mean(predicted == test_opinion))


· SVM自动分类器预测结果

打印预测结果：
['手机银行-安全工具-交易限额' '手机银行-安全工具-交易限额' '手机银行-安全工具-交易限额' '手机银行-安全工具-交易限额'
 '手机银行-安全工具-交易限额' '手机银行-安全工具-交易限额' '手机银行-安全工具-交易限额' '手机银行-安全工具-交易限额'
 '手机银行-安全工具-交易限额' '手机银行-安全工具-交易限额' '手机银行-安全工具-设备绑定与解绑' '手机银行-转账汇款-无法转账'
 '手机银行-账户管理-关联账户' '手机银行-账户管理-关联账户' '手机银行-账户管理-关联账户' '手机银行-账户管理-关联账户'
 '手机银行-账户管理-关联账户' '手机银行-账户管理-关联账户' '手机银行-账户管理-关联账户' '手机银行-账户管理-关联账户'
 '手机银行-账户管理-关联账户' '手机银行-转账汇款-款项未到账' '手机银行-转账汇款-款项未到账' '手机银行-转账汇款-款项未到账'
 '手机银行-转账汇款-款项未到账' '手机银行-转账汇款-款项未到账' '手机银行-转账汇款-无法转账' '手机银行-转账汇款-款项未到账'
 '手机银行-转账汇款-款项未到账' '手机银行-转账汇款-款项未到账' '手机银行-转账汇款-款项未到账' '手机银行-转账汇款-无法转账'
 '手机银行-转账汇款-无法转账' '手机银行-转账汇款-无法转账' '手机银行-转账汇款-无法转账' '手机银行-转账汇款-无法转账'
 '手机银行-转账汇款-款项被退回' '手机银行-转账汇款-款项未到账' '手机银行-转账汇款-款项被退回' '手机银行-转账汇款-款项被退回']
============================================
计算预测结果精确度
SVC 0.9



         
    </div>
  </div>

  
 	  <div class="post hidden">
    <h1 class="post-title">
      <a href="/anlp/2017/08/09/nlp01/">
        HMM隐马尔可夫模型的实例
      </a>
    </h1>
    <span class="post-date" >09 Aug 2017</span>
    <div class="post-content">
         
          1.引言

  最近看到许多同学对自然语言处理（NLP）都非常感兴趣，但对于NLP中一些基本的统计模型并不够了解。遂写下该博文，帮助大家掌握NLP中文分词这一大板块比较核心的统计模型——HMM隐马尔可夫模型。    
  HMM模型的全称是Hidden Markov Model，看关键词就知道该模型中存在隐含层，它是用来描述一个含有隐含未知参数的马尔可夫过程，其目的是希望通过求解这些隐含的参数来进行实体识别，说简单些也就是起到词语粘合的作用。    
  举一个经典的实例：一个北京的朋友每天根据天气【下雨，天晴】决定当天的活动【公园散步,购物,清理房间】中的一种，我每天只能在朋友圈上看到她发的消息 “我前天公园散步，昨天购物，今天清理房间了！”，那么我如何根据她发的消息推断北京这三天的天气？

2.隐马尔可夫模型概述

  任何一个HMM模型都包括如下五方面：   
  Obs 显现层   
  States 隐含层   
  Start_p 初始概率   
  Trans_p 转移概率   
  Emit_p 发射概率
&lt;div align="center"&gt;   
 
图：HMM隐马尔可夫模型变迁图
&lt;/div&gt;

3.样例计算

命题：“我前天公园散步，昨天购物，今天清理房间了！”  

HMM模型的计算公式：

# 隐含层
states = ('Rainy', 'Sunny')
# 显现层
observations = ('walk', 'shop', 'clean')
# 初始概率
start_probability = {'Rainy': 0.6, 'Sunny': 0.4}
# 转移概率
transition_probability = {
    'Rainy': {'Rainy': 0.7, 'Sunny': 0.3},
    'Sunny': {'Rainy': 0.4, 'Sunny': 0.6},
}
# 发射概率
emission_probability = {
    'Rainy': {'walk': 0.1, 'shop': 0.4, 'clean': 0.5},
    'Sunny': {'walk': 0.6, 'shop': 0.3, 'clean': 0.1},
}


【第一天】【散步】= [初始概率,下雨] * [发射概率,散步] = 0.6*0.1 = 0.06   
【第一天】【散步】= [初始概率,晴天] * [发射概率,散步] = 0.4*0.6 = 0.24     
因为0.24&gt;0.06，所以第一天可能是 晴天

【第二天】【购物】= [初始概率,晴天] * [转移概率,M=&gt;下雨] * [发射概率,购物] = 0.24*0.4*0.4= 0.0384   
【第二天】【购物】= [初始概率,晴天] * [转移概率,M=&gt;晴天] * [发射概率,购物] = 0.24*0.6*0.3= 0.0432   
【第二天】【购物】= [初始概率,下雨] * [转移概率,M=&gt;下雨] * [发射概率,购物] = 0.06*0.7*0.4= 0.0168      
【第二天】【购物】= [初始概率,下雨] * [转移概率,M=&gt;晴天] * [发射概率,购物] = 0.06*0.3*0.3= 0.0054   
需要注意的是，这里0.0432是累积概率 ，所以全局最优解： 第一天 晴天；第二天 晴天（不能够理解这句话的同学请继续看第三天）   

【第三天】【清理】= [初始概率,晴天,下雨] * [转移概率,M=&gt;下雨] * [发射概率,清理] = 0.0384*0.7*0.5= 0.01344   
【第三天】【清理】= [初始概率,晴天,下雨] * [转移概率,M=&gt;晴天] * [发射概率,清理] = 0.0384*0.3*0.1= 0.00115   
【第三天】【清理】= [初始概率,晴天,晴天] * [转移概率,M=&gt;下雨] * [发射概率,清理] = 0.0432*0.4*0.5= 0.00864    
【第三天】【清理】= [初始概率,晴天,晴天] * [转移概率,M=&gt;晴天] * [发射概率,清理] = 0.0432*0.6*0.1= 0.00259   
【第三天】【清理】= [初始概率,下雨,下雨] * [转移概率,M=&gt;下雨] * [发射概率,清理] = 0.0168*0.7*0.5= 0.00588  
【第三天】【清理】= [初始概率,下雨,下雨] * [转移概率,M=&gt;晴天] * [发射概率,清理] = 0.0168*0.3*0.1= 0.00050  
【第三天】【清理】= [初始概率,下雨,晴天] * [转移概率,M=&gt;下雨] * [发射概率,清理] = 0.0054*0.4*0.5= 0.00108  
【第三天】【清理】= [初始概率,下雨,晴天] * [转移概率,M=&gt;晴天] * [发射概率,清理] = 0.0054*0.6*0.1= 0.00032   
从这里就能看出，累积概率最大值为0.01344，所以全局最优解：第一天晴天；第二天 下雨；第三天 下雨  
注：若认为第二天应该是晴天的同学，请区分全局最优解和局部最优解

Python程序实现

# Python -version 3.5以上版本

# 打印路径概率表
def print_dptable(V):
    print ("    ",)
    for i in range(len(V)):
        print ("%7d" % i,)
    print ()
    for y in V[0].keys():
        print ("%.5s: " % y,)
        for t in range(len(V)):
            print ("%.7s" % ("%f" % V[t][y]),)
        print ()

def viterbi(obs, states, start_p, trans_p, emit_p):
    # 路径概率表 V[时间][隐含层] = 概率
    V = [{}]
    # 中间变量
    path = {}
    # 状态初始化 (t == 0)
    for y in states:
        V[0][y] = start_p[y] * emit_p[y][obs[0]]
        path[y] = [y]
    # 对 t &gt; 0 跑一遍维特比算法
    for t in range(1, len(obs)):
        V.append({})
        newpath = {}
        for y in states:
            # 概率 隐含层 =  前状态是y0的初始概率 * y0转移到y的转移概率 * y表现为当前状态的发射概率
            (prob, state) = max([(V[t - 1][y0] * trans_p[y0][y] * emit_p[y][obs[t]], y0) for y0 in states])
            # 记录最大概率
            V[t][y] = prob
            # 记录路径
            newpath[y] = path[state] + [y]
        path = newpath
    print_dptable(V)
    (prob, state) = max([(V[len(obs) - 1][y], y) for y in states])
    return (prob, path[state])

# HMM 实例导入
states = ('Rainy', 'Sunny')
observations = ('walk', 'shop', 'clean')
start_probability = {'Rainy': 0.6, 'Sunny': 0.4}
transition_probability = {
    'Rainy': {'Rainy': 0.7, 'Sunny': 0.3},
    'Sunny': {'Rainy': 0.4, 'Sunny': 0.6},
}
emission_probability = {
    'Rainy': {'walk': 0.1, 'shop': 0.4, 'clean': 0.5},
    'Sunny': {'walk': 0.6, 'shop': 0.3, 'clean': 0.1},
}

def example():
    #将实例值传输到viterbi函数
    return viterbi(observations,
                   states,
                   start_probability,
                   transition_probability,
                   emission_probability
                   )
print (example())



         
    </div>
  </div>

  
 	  <div class="post hidden">
    <h1 class="post-title">
      <a href="/anlp/2017/06/11/nlp03/">
        Python之 结巴中文分词
      </a>
    </h1>
    <span class="post-date" >11 Jun 2017</span>
    <div class="post-content">
         
            结巴分词（Jieba）是python中一个比较常用的中文分词包，功能包括：中文分词、词性标注、未登录词识别等。本文将着重介绍以下两个部分：Part A. 结巴分词下载与安装; Part B. 结巴中文分词基础应用。

Part A 结巴分词下载与安装

1.下载结巴分词（jieba）   
 官方下载地址：https://pypi.python.org/pypi/jieba/   
 注：可尝试官网中所述的其他安装方法

2.将下载好的ZIP包，解压缩到新建文件夹中

3.进入Windows下cmd命令框，输入如下命令

C:\Users\Admin&gt;e:
E:\&gt;cd 新建文件夹
E:\新建文件夹&gt;cd jieba-0.38
D:\新建文件夹\jieba-0.38&gt;python setup.py install
#=========== 安装完成 ============# 


Part B 结巴中文分词基础应用

 结巴分词共支持如下三种分词模式：   
 精确模式 也是最常用的模式，适合文本分析；   
 全模式 把句子中所有的可以组成词的词语都切分出来；   
 搜索引擎模式 在精确模式的基础上，对长词再次切分，从而提高召回率；  

 Python程序设计

import jieba
text="小明硕士毕业于中国科学院计算所，后在日本京都大学深造"

seg_list = jieba.cut(text, cut_all = False)  
print("Precise Mode: " + "/".join(seg_list))  #精确模式

seg_list = jieba.cut(text, cut_all=True)  
print("Full Mode: " + "/ ".join(seg_list))  # 全模式 

seg_list = jieba.cut_for_search(text)  #搜索引擎模式  
print("Search Mode: " + "/".join(seg_list)) 


 Python执行结果

Precise Mode: 小明/硕士/毕业/于/中国科学院/计算所/，/后/在/日本京都大学/深造
Full Mode: 小/ 明/ 硕士/ 毕业/ 于/ 中国/ 中国科学院/ 科学/ 科学院/ 学院/ 计算/ 计算所/ / / 后/ 在/ 日本/ 日本京都大学/ 京都/ 京都大学/ 大学/ 深造
Search Mode: 小明/硕士/毕业/于/中国/科学/学院/科学院/中国科学院/计算/计算所/，/后/在/日本/京都/大学/日本京都大学/深造


         
    </div>
  </div>

  
  <script src="/public/js/pagination.js"></script>

</div>

			</div>
		</div>

		<label for="sidebar-checkbox" class="sidebar-toggle"></label>

		<script>
			(function(document) {
				var toggle = document.querySelector('.sidebar-toggle');
				var sidebar = document.querySelector('#sidebar');
				var checkbox = document.querySelector('#sidebar-checkbox');

				document.addEventListener('click', function(e) {
					var target = e.target;
					if(!checkbox.checked ||
						sidebar.contains(target) ||
						(target === checkbox || target === toggle)) return;

					checkbox.checked = false;
				}, false);
				
				var article = document.getElementById('articles');
				var isExpanded = article.getAttribute('expanded');
				var categoryName = document.getElementsByName('category-name');
				article.addEventListener('click', function(e) {
					if(isExpanded){
						isExpanded = false;
						for (var i=0;i<categoryName.length;i++){
							categoryName[i].setAttribute('class','sidebar-nav-item category-name-hide');
						}					
					}else{
						isExpanded = true;
						for (var i=0;i<categoryName.length;i++){
							categoryName[i].setAttribute('class','sidebar-nav-item category-name-show');
						}
					}
				}, false);
			})(document);
			$('#search-btn').on('click',function(){
				$('#search-bar').submit();
			});
		</script>
	<script src="/public/js/typeahead-custom.js"></script>
	</body>

</html>