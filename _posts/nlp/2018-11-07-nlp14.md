---
layout: post
title: BERT:用于语义理解的深度双向预训练Transformer
category: anlp
---
&emsp;&emsp;鉴于BERT最近特别火，但相关中文资料却特别少，因此将BERT的论文理论部分（1-3节）翻译成中文以方便大家研读。        

<div align="center">
<img width="500" height="250" src="https://raw.githubusercontent.com/carrylaw/IMG/master/img/sucai47.jpg" /> 
</div> 

**<span style="color:#008B8B;">·&emsp;&nbsp;摘要</span>**     
&emsp;&emsp;我们将来介绍一个名为BERT的模型。与现有语言模型不同的是，BERT旨在通过调节所有层中的上下文来进行深度双向的预训练。因此，预训练的BERT表示可以通过另外的输出层进行调整，以创建用于广泛任务的状态模型，例如问题转换和语言参考，而无需实质的任务特定体系结构修改。             
&emsp;&emsp;BERT在概念上简单且经验丰富。 它在11项自然语言处理任务中获得了最新的最新成果，包括将GLUE基准推至80.4％（绝对改进率7.6％），MultiNLI精度达到86.7％（绝对改进5.6％）和SQuAD v1.1 问题回答测试F1到93.2（1.5绝对改进），优于人类表现2.0。    

**<span style="color:#008B8B;">1&emsp;&nbsp;引言</span>**        
&emsp;&emsp;语言模型预训练已证明可有效改进许多自然语言处理任务。这些任务包括句子级任务，如自然语言推理和释义，旨在通过整体分析句子来预测句子之间的关系，以及令牌级任务，如命名实体识别和SQuAD问答，其中模型需要在令牌级别产生细粒度输出。         
&emsp;&emsp;将预训练语言表示应用于下游任务有两种现有策略：基于特征和微调。基于特征的方法使用任务特定的体系结构，其包括预先训练的表示作为附加特征。基于微调的方法引入了最小的任务特定参数，并通过简单地调整预训练参数来训练下游任务。 在以前的工作中，两种方法在预训练期间共享相同的目标函数，其中使用单向语言模型来学习一般语言表示。         
&emsp;&emsp;我们认为当前的技术严重限制了预训练表示的力量，特别是对于微调方法。 主要限制是标准语言模型是单向的，这限制了在预训练期间可以使用的体系结构的选择。 例如，在OpenAI GPT中，作者使用左右架构，其中每个令牌只能处理Transformer的自我关注层中的先前令牌。 这些限制对于句子级别的任务来说是次优的，并且在应用基于调整的语言级别任务（例如SQuAD问答）时可能是毁灭性的，其中从两个方向合并上下文是至关重要的。            
&emsp;&emsp;在本文中，我们通过提出BERT来改进基于微调的方法。BERT受到完形填空任务的启发提出“掩盖语言模型（MLM）”，该模型是通过一个新的预训练目标来解决前面提到的单向约束。被掩盖的语言模型从输入中随机地掩盖一些标记，并且客观主义者仅基于其上下文来预测被掩盖的单词的原始词汇。 与从左到右的语言模型预训练不同，MLM目标允许表示融合左右上下文，这允许我们预训练深度双向变换器。除了掩盖语言模型，我们还引入了一个“下一句预测”任务，它共同预先训练文本对表示。             
&emsp;&emsp;本文主要贡献如下：          
·&emsp;&nbsp;证明了双向预训练对语言表达的重要性。与使用单向语言模型进行预训练不同，BERT使用掩蔽语言模型来实现预训练的深度双向表示。          
·&emsp;&nbsp;预先训练的表示消除了许多重型工程任务特定结构的需求。BERT是第一个基于调整的表示模型，它在大量句子级和token级任务上实现了最先进的性能，优于许多具有任务特定体系结构的系统。       
·&emsp;&nbsp;BERT刷新了11项NLP任务的性能记录。本文还报告了 BERT 的模型简化研究（ablation study），表明模型的双向性是一项重要的新成果。       
  

**<span style="color:#008B8B;">2&emsp;&nbsp;文献回顾</span>**          
&emsp;&emsp;语言表达的预训练经过很长一段历史，在本节我们简要回顾一下。        
**<span style="color:#008B8B;">2.1&emsp;&nbsp;基于特征的方法</span>**                   
&emsp;&emsp;几十年来，学习广泛适用的词语表达一直是研究的一个活跃领域，包括非神经和神经方法。 预训练的单词嵌入被认为是现代自由系统的一个组成部分，对从头开始学习的嵌入进行了重大改进。 这些方法已被推广到较粗糙的粒度，例如句子嵌入或段落嵌入。 与传统的单词嵌入一样，这些学习的表示通常也用作下游模型中的特征。 ELMo将传统的单词嵌入研究概括为不同的维度。 他们建议从语言模型中提取上下文敏感特征。 当将上下文词嵌入与现有的任务特定体系结构集成时，ELMo推进了几个主要NLP基准测试的最新技术，包括SQUAD上的问题回答，情感分析和命名实体识别。       
&emsp;&emsp;这些方法已被推广到较粗糙的粒度，例如句子嵌入或段落嵌入。 与传统的单词嵌入一样，这些学习的表示通常也用作下游模型中的特征。       
&emsp;&emsp;ELMo将传统的单词嵌入研究概括为不同的维度。 他们建议从语言模型中提取上下文敏感特征。 当将上下文词嵌入与现有的任务特定体系结构集成时，ELMo推进了几个主要NLP基准测试的最新技术，包括SQUAD上的问题回答，情感分析和命名实体识别。     
**<span style="color:#008B8B;">2.2&emsp;&nbsp;基于微调的方法</span>**           
&emsp;&emsp;从语言模型（LMs）转移学习的最新趋势是在对用于监督下游任务的相同模型进行微调之前，在LM目标上预先训练一些模型架构。 这些方法的优点是需要从头开始学习参数。 至少部分由于这一优势，OpenAI GPT在GLUE基准测试的许多句子级别任务中获得了先前最先进的结果。     
**<span style="color:#008B8B;">2.3&emsp;&nbsp;从监督数据转移学习</span>**      
&emsp;&emsp;虽然无监督预训练的优势在于可用数据几乎无限，但也有工作表明从具有大数据集的监督任务中有效转移，例如自然语言推断和机器翻译。 在NLP之外，计算机视觉研究也证明了从大型预训练模型转移学习的重要性，其中一个有效的方法是对在ImageNet上预训练的模型进行微调。      

**<span style="color:#008B8B;">3&emsp;&nbsp;BERT</span>**     
&emsp;&emsp;我们在本节介绍BERT及其详细实现。首先介绍BERT的模型架构和输入表示。 然后，我们将在3.3节中介绍预培训任务，即本文的核心创新。 预训练程序和微调程序分别在第3.4节和第3.5节中详述。 最后，第3.6节讨论了BERT和OpenAI GPT之间的差异。    
**<span style="color:#008B8B;">3.1&emsp;&nbsp;模型架构</span>**    
&emsp;&emsp;BERT的模型架构是基于Vaswani等人描述的原始实现的多层双向变换器编码器，发布于tensor2tensor库。由于现有变换器实现与原始实现完全相同，我们将省略对模型体系结构的详尽背景描述，并向读者推荐Vaswani等人的论文。    
&emsp;&emsp;在这项工作中，我们将层数（即变换器块）表示为L，将隐藏大小表示为H，将自我关注头的数量表示为A.在所有情况下，我们将前馈/过滤器大小设置为 4H，即H = 768为3072，H = 1024为4096.我们主要报告两种模型尺寸的结果：    
&emsp;&emsp;**BERT_BASE: L=12, H=768, A=12, Total Parameters=110M**      
&emsp;&emsp;**BERT_LARGE: L=24, H=1024, A=16, Total Parameters=340M**       
&emsp;&emsp;为了公平比较，选择BERT_BASE与OpenAI GPT具有相同的模型大小。 然而，重要的是，BERT变换器使用双向自我关注，而GPT变换器使用受限制的自我关注，其中每个令牌只能处理其左侧的上下文。 我们注意到，在文献中，双向变换器通常被称为“变换器编码器”，而左上下文仅被称为“变换器解码器”，因为它可以用于文本生成。 BERT，OpenAI GPT和ELMo之间的比较如图1所示。    

<div align="center">
<img width="500" height="250" src="https://raw.githubusercontent.com/carrylaw/IMG/master/img/sucai47.jpg" /> 
</div> 

&emsp;&emsp;图1：训练前模型架构的差异。 BERT使用双向变换器。 OpenAI GPT使用从左到右的变换器。 ELMo使用经过独立训练的从左到右和从右到左LSTM的串联来生成下游任务的功能。 在三个中，只有BERT表示在所有层中共同依赖于左右上下文。

**<span style="color:#008B8B;">3.2&emsp;&nbsp;输入表示</span>**  
&emsp;&emsp;我们的输入表示能够在一个标记序列中明确地表示单个文本句子或一对文本句子（例如，[问题，答案]）。对于给定标记，其输入表示通过对相应标记求和来构造， 段和位置嵌入。 图2给出了输入表示的直观表示。   

<div align="center">
<img width="500" height="250" src="https://raw.githubusercontent.com/carrylaw/IMG/master/img/sucai47.jpg" /> 
</div>

&emsp;&emsp;图2：BERT输入表示。 关于嵌入式，嵌入式嵌入和位置嵌入的信息      

**我们使用带有30,000个令牌词汇表的WordPiece嵌入。 我们用##表示分词。     
我们使用学习的位置嵌入，支持的序列长度最多为512个令牌。      
每个序列的第一个标记始终是特殊的分类嵌入（[CLS]）。 对应于该令牌的最终隐藏状态（即，Transformer的输出）被用作分类任务的聚合序列表示。对于非分类任务，此向量将被忽略。      
句子对被打包成一个单独的序列。 我们以两种方式区分句子。 首先，我们用特殊标记（[SEP]）将它们分开。 其次，我们添加一个学习句子A嵌入到第一个句子的每个句子和一个句子B嵌入到第二个句子的每个标记。         
对于单句输入，我们只使用句子A嵌入。**

**<span style="color:#008B8B;">3.3&emsp;&nbsp;预训练任务</span>**  
与众不同的是，我们不使用传统的从左到右或从右到左的语言模型来预训练BERT。相反，我们使用两个新的无监督预测任务预训练BERT。
**<span style="color:#008B8B;">3.3.1&emsp;&nbsp;任务1：蒙面语言模型</span>**      
直观地说，有理由相信深度双向模型比左向右模型或从左到右和右宽容模型的浅层连接更严格。遗憾的是，标准条件语言模型只能从左到右或从右到左进行训练，因为双向调节将允许每个单词在多层上下文中间接地“看到自己”。     
为了训练非常双向的表示，我们采用一种直接的方法来掩盖输入令牌的一些百分比年龄，并且然后仅仅预测了掩码。我们将此过程称为“蒙面LM”（MLM），尽管它在文献中通常被称为完形任务。在这种情况下，对应于掩码令牌的最终隐藏向量被馈送到词汇表上的输出softmax，如在标准LM中。在我们的所有实验中，我们随机地屏蔽每个序列中所有WordPiece标记的15％。与去噪自动编码器相比，我们只预测掩蔽的单词而不是重建整个输入。       
虽然这确实允许我们获得双向预训练模型，但这种方法有两个缺点。 首先，我们正在创建预训练和微调之间的不匹配，因为在调整期间从未看到[MASK]令牌。 为了缓解这个问题，我们并不总是用实际的[MASK]令牌替换“蒙面”字。 相反，训练用户会随机选择15％的代币，例如，在句子中，我的狗毛茸茸，选择毛茸茸。 然后执行以下过程：
