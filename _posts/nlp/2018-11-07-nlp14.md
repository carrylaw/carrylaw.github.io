---
layout: post
title: BERT:用于语义理解的深度双向预训练转换器（Transformer）
category: anlp
---
&emsp;&emsp;鉴于最近BERT在人工智能领域特别火，但相关中文资料却很少，因此将[BERT论文](https://github.com/carrylaw/Archive/blob/master/NLP%E6%96%87%E4%BB%B6%E5%A4%B9/BERT.pdf)理论部分（1-3节）翻译成中文以方便大家后续研究。        

<div align="center">
<img width="500" height="250" src="https://raw.githubusercontent.com/carrylaw/IMG/master/img_nlp/sucai25.jpg" /> 
</div> 

**<span style="color:#008B8B;">·&emsp;&nbsp;摘要</span>**     
&emsp;&emsp;本文主要介绍一个名为BERT的模型。与现有语言模型不同的是，BERT旨在通过调节所有层中的上下文来进行深度双向的预训练。因此，预训练的BERT表示可以通过另外的输出层进行调整，以创建用于广泛任务的状态模型，例如问题转换和语言参考，而无需实质的任务特定体系结构修改。             
&emsp;&emsp;BERT在概念上简单且经验丰富。 它在11项自然语言处理任务中获得了最新的最新成果，包括将GLUE基准推至80.4％（绝对改进率7.6％），MultiNLI精度达到86.7％（绝对改进5.6％）和SQuAD v1.1 问题回答测试F1到93.2。    

**<span style="color:#008B8B;">1&emsp;&nbsp;引言</span>**        
&emsp;&emsp;语言模型预训练已证明可有效改进许多自然语言处理任务。这些任务包括句子级任务，如自然语言推理和释义，旨在通过整体分析句子来预测句子之间的关系，以及token级任务，如命名实体识别和SQuAD问答，其中模型需要在token级别产生细粒度输出。         
&emsp;&emsp;将预训练语言表示应用于下游任务有两种现有策略：基于特征和微调。基于特征的方法使用任务特定的体系结构，其包括预先训练的表示作为附加特征。基于微调的方法引入了最小的任务特定参数，并通过简单地调整预训练参数来训练下游任务。 在以前的工作中，两种方法在预训练期间共享相同的目标函数，其中使用单向语言模型来学习一般语言表示。         
&emsp;&emsp;我们认为当前的技术严重限制了预训练表示的力量，特别是对于微调方法。 主要限制是标准语言模型是单向的，这限制了在预训练期间可以使用的体系结构的选择。 例如，在OpenAI GPT中，作者使用左右架构，其中每个token只能处理Transformer的自我关注层中的先前token。 这些限制对于句子级别的任务来说是次优的，并且在应用基于调整的语言级别任务（例如SQuAD问答）时可能是毁灭性的，其中从两个方向合并上下文是至关重要的。            
&emsp;&emsp;在本文中，我们通过提出BERT来改进基于微调的方法。BERT受到完形填空任务的启发提出“掩盖语言模型（MLM）”，该模型是通过一个新的预训练目标来解决前面提到的单向约束。被掩盖的语言模型从输入中随机地掩盖一些标记，并且客观主义者仅基于其上下文来预测被掩盖的单词的原始词汇。 与从左到右的语言模型预训练不同，MLM目标允许表示融合左右上下文，这允许我们预训练深度双向变换器。除了掩盖语言模型，我们还引入了一个“下一句预测”任务，它共同预先训练文本对表示。             
&emsp;&emsp;本文主要贡献如下：          
·&emsp;&nbsp;证明了双向预训练对语言表达的重要性。与使用单向语言模型进行预训练不同，BERT使用掩蔽语言模型来实现预训练的深度双向表示。          
·&emsp;&nbsp;预先训练的表示消除了许多重型工程任务特定结构的需求。BERT是第一个基于调整的表示模型，它在大量句子级和token级任务上实现了最先进的性能，优于许多具有任务特定体系结构的系统。       
·&emsp;&nbsp;BERT刷新了11项NLP任务的性能记录。本文还报告了 BERT 的模型简化研究（ablation study），表明模型的双向性是一项重要的新成果。       
  
**<span style="color:#008B8B;">2&emsp;&nbsp;文献回顾</span>**          
&emsp;&emsp;语言表达的预训练经过很长一段历史，在本节我们简要回顾一下。（注: 本节为预备知识，想深入了解的同学可自行研究一下。）        
**<span style="color:#008B8B;">2.1&emsp;&nbsp;基于特征的方法 Feature-basedApproaches</span>**                   
&emsp;&emsp;预训练的word embedding被认为是现代 NLP 系统中不可或缺的一部分，与从头学习的 embedding 相比提供了显着的改进。基于 word embedding 这些方法，已经推广出了 sentence embedding，paragraph embedding 等多种方法。与传统的word embedding一样，这些学习的表示通常也用作下游模型中的特征。 ELMo将传统的word embedding研究概括为不同的维度。 他们建议从语言模型中提取上下文敏感特征。当将上下文词嵌入与现有的任务特定体系结构集成时，ELMo推进了几个主要NLP基准测试的最新技术，包括SQUAD上的问题回答，情感分析和命名实体识别。           
**<span style="color:#008B8B;">2.2&emsp;&nbsp;基于微调的方法 Fine-tuningApproaches</span>**           
&emsp;&emsp;从语言模型（LMs）转移学习的最新趋势是在对用于监督下游任务的相同模型进行微调之前，在LM目标上预先训练一些模型架构。 这些方法的优点是需要从头开始学习参数。 至少部分由于这一优势，OpenAI GPT在GLUE基准测试的许多句子级别任务中获得了先前最先进的结果。          
**<span style="color:#008B8B;">2.3&emsp;&nbsp;从监督数据转移学习 TransferLearningfromSupervisedData</span>**      
&emsp;&emsp;虽然无监督预训练的优势在于可用数据几乎无限，但也有工作表明从具有大数据集的监督任务中有效转移，例如自然语言推断和机器翻译。 在NLP之外，计算机视觉研究也证明了从大型预训练模型转移学习的重要性，其中一个有效的方法是对在ImageNet上预训练的模型进行微调。      

**<span style="color:#008B8B;">3&emsp;&nbsp;BERT</span>**     
&emsp;&emsp;论文在本节介绍BERT及其详细实现。首先介绍BERT的模型架构和输入表示。 然后，我们将在3.3节中介绍预培训任务，即本文的核心创新。 预训练程序和微调程序分别在第3.4节和第3.5节中详述。 最后，第3.6节讨论了BERT和OpenAI GPT之间的差异。    
**<span style="color:#008B8B;">3.1&emsp;&nbsp;模型架构 Model Architecture</span>**    
&emsp;&emsp;BERT的模型架构是基于Vaswani等人描述的原始实现的多层双向变换器编码器，并发布于tensor2tensor库。由于Transformer的使用最近变得无处不在，论文中的实现与原始实现完全相同，因此这里将省略对模型结构的详细描述。      
&emsp;&emsp;在这项工作中，我们将层数（即Transformer blocks）表示为L，将隐藏大小表示为H，将self-attention heads的数量表示为A. 在所有情况下，将feed-forward/filter 的大小设置为 4H，即H = 768时为3072，H = 1024时为4096。论文主要报告了两种模型大小的结果：     
&emsp;&emsp;**BERT_BASE: L=12, H=768, A=12, Total Parameters=110M**      
&emsp;&emsp;**BERT_LARGE: L=24, H=1024, A=16, Total Parameters=340M**       
&emsp;&emsp;为了进行比较，选择与BERT_BASE具有相同的模型大小的OpenAI GPT。 然而，重要的是，BERT变换器使用双向self-attention，而GPT变换器使用受限制的self-attention，其中每个token只能处理其左侧的上下文。 我们注意到，在文献中，双向 Transformer通常被称为“Transformer encoder”，而左上下文仅被称为“Transformer encoder”，因为它可以用于文本生成。 BERT，OpenAI GPT和ELMo之间的比较如图1所示。     

<div align="center">
<img width="800" height="250" src="https://raw.githubusercontent.com/carrylaw/IMG/master/img_nlp/sucai26.png" /> 
</div> 

&emsp;&emsp;图1：训练前模型架构的差异。 BERT使用双向变换器。 OpenAI GPT使用从左到右的变换器。 ELMo使用经过独立训练的从左到右和从右到左LSTM的串联来生成下游任务的功能。 在三个中，只有BERT表示在所有层中共同依赖于左右上下文。

**<span style="color:#008B8B;">3.2&emsp;&nbsp;输入表示 Input Representation</span>**  
&emsp;&emsp;我们的输入表示能够在一个标记序列中明确地表示单个文本句子或一对文本句子（例如，[Question, Answer]）。对于给定token，其输入表示通过对相应的token、segment和position embeddings进行求和来构造。图2是输入表示的直观表示：       

<div align="center">
<img width="800" height="250" src="https://raw.githubusercontent.com/carrylaw/IMG/master/img_nlp/sucai27.png" /> 
</div>

&emsp;&emsp;图2：BERT输入表示。 输入嵌入是token embeddings, segmentation embeddings 和position embeddings 的总和。      

&emsp;&emsp;具体如下：      
·&emsp;&nbsp;使用WordPiece嵌入（Wu et al., 2016）和30,000个token的词汇表。用##表示分词。        
·&emsp;&nbsp;使用学习的positional embeddings，支持的序列长度最多为512个token。          
·&emsp;&nbsp;每个序列的第一个token始终是特殊分类嵌入（[CLS]）。对应于该token的最终隐藏状态（即，Transformer的输出）被用作分类任务的聚合序列表示。对于非分类任务，将忽略此向量。       
·&emsp;&nbsp;句子对被打包成一个序列。以两种方式区分句子。首先，用特殊标记（[SEP]）将它们分开。其次，添加一个learned sentence A嵌入到第一个句子的每个token中，一个sentence B嵌入到第二个句子的每个token中。          
·&emsp;&nbsp;对于单个句子输入，只使用 sentence A嵌入。       

**<span style="color:#008B8B;">3.3&emsp;&nbsp;预训练任务</span>**      
&emsp;&emsp;与众不同的是，我们不使用传统的从左到右或从右到左的语言模型来预训练BERT。相反，我们使用两个新的无监督预测任务预训练BERT。     
**<span style="color:#008B8B;">3.3.1&emsp;&nbsp;任务1：遮掩语言模型  Masked LM</span>**        
&emsp;&emsp;从直觉上看，研究团队有理由相信，深度双向模型比left-to-right模型或left-to-right and right-to-left模型的浅层连接更强大。遗憾的是，标准条件语言模型只能从左到右或从右到左进行训练，因为双向条件作用将允许每个单词在多层上下文中间接地“see itself”。                
&emsp;&emsp;为了训练一个深度双向表示（deep bidirectional representation），研究团队采用了一种简单的方法，即随机屏蔽（masking）部分输入token，然后只预测那些被屏蔽的token。论文将这个过程称为“masked LM”(MLM)，尽管在文献中它经常被称为Cloze任务(Taylor, 1953)。                    
&emsp;&emsp;在这个例子中，与masked token对应的最终隐藏向量被输入到词汇表上的输出softmax中，就像在标准LM中一样。在团队所有实验中，随机地屏蔽了每个序列中15%的WordPiece token。与去噪的自动编码器（Vincent et al.， 2008）相反，只预测masked words而不是重建整个输入。                
&emsp;&emsp;虽然这确实能让团队获得双向预训练模型，但这种方法有两个缺点。首先，预训练和finetuning之间不匹配，因为在finetuning期间从未看到[MASK]token。为了解决这个问题，团队并不总是用实际的[MASK]token替换被“masked”的词汇。相反，训练数据生成器随机选择15％的token。例如在这个句子“my dog is hairy”中，它选择的token是“hairy”。然后，执行以下过程：                    
&emsp;&emsp;数据生成器将执行以下操作，而不是始终用[MASK]替换所选单词：             
·&emsp;&nbsp;80％的时间：用[MASK]标记替换单词，例如，my dog is hairy → my dog is [MASK]           
·&emsp;&nbsp;10％的时间：用一个随机的单词替换该单词，例如，my dog is hairy → my dog is apple            
·&emsp;&nbsp;10％的时间：保持单词不变，例如，my dog is hairy → my dog is hairy.这样做的目的是将表示偏向于实际观察到的单词。               
&emsp;&emsp;ransformer encoder不知道它将被要求预测哪些单词或哪些单词已被随机单词替换，因此它被迫保持每个输入token的分布式上下文表示。此外，因为随机替换只发生在所有token的1.5％（即15％的10％），这似乎不会损害模型的语言理解能力。          
&emsp;&emsp;使用MLM的第二个缺点是每个batch只预测了15％的token，这表明模型可能需要更多的预训练步骤才能收敛。团队证明MLM的收敛速度略慢于 left-to-right的模型（预测每个token），但MLM模型在实验上获得的提升远远超过增加的训练成本。                
**<span style="color:#008B8B;">3.3.2&emsp;&nbsp;任务2：下一句预测 Next Sentence Prediction</span>**      
&emsp;&emsp;许多重要的下游任务，如问答（QA）和自然语言推理（NLI）都是基于理解两个句子之间的关系，这并没有通过语言建模直接获得。       
&emsp;&emsp;在为了训练一个理解句子的模型关系，预先训练一个二进制化的下一句测任务，这一任务可以从任何单语语料库中生成。具体地说，当选择句子A和B作为预训练样本时，B有50％的可能是A的下一个句子，也有50％的可能是来自语料库的随机句子。例如：        
&emsp;&emsp;Input = [CLS] the man went to [MASK] store [SEP]       
&emsp;&emsp;he bought a gallon [MASK] milk [SEP]       
&emsp;&emsp;Label = IsNext       
&emsp;&emsp;Input = [CLS] the man [MASK] to the store [SEP]        
&emsp;&emsp;penguin [MASK] are flight ##less birds [SEP]      
&emsp;&emsp;Label = NotNext         
&emsp;&emsp;团队完全随机地选择了NotNext语句，最终的预训练模型在此任务上实现了97％-98％的准确率。       

**<span style="color:#008B8B;">3.4&emsp;&nbsp;预训练程序</span>**        
&emsp;&emsp;预训练程序主要遵循现有的语言模型预训练文献。对于训练前语料库，我们使用BooksCorpus（800M单词）和英语维基百科（2,500M单词）的串联。对于维基百科，我们只提取文本段落并忽略列表，表格和标题。使用文档级语料库而不是诸如Billion Word Benchmark之类的句子级语料库以提取长的连续序列是至关重要的。        
&emsp;&emsp;为了生成每个训练输入序列，我们从语料库中采样两个文本跨度，我们将其称为“句子”，即使它们通常比单个句子长得多（但也可以更短）。第一个句子接收A嵌入，第二个句子接收B嵌入。 50％的时间B是跟随A的实际下一个句子，50％的时间是随机句子，这是为“下一句话预测”任务完成的。对它们进行采样，使得组合长度≤512个token。在WordPiece标记化之后应用LM掩蔽，具有15％的统一掩蔽率，并且不特别考虑部分字块。      
&emsp;&emsp;我们训练批量大小为256个序列（256个序列* 512个token= 128,000个token/批次），持续1,000,000个步骤，这比33亿个词语料库大约40个时期。我们使用Adam的学习率为1e-4，β1= 0.9，β2= 0.999，L2权重衰减为0.01，学习率在前10,000步的预热，以及线性衰减的学习率。在所有层上的Weuseadropout概率为0.1。在OpenAI GPT之后，我们使用gelu激活（Hendrycks和Gimpel，2016）而不是标准relu。        
&emsp;&emsp;训练损失是平均掩盖的LM可能性和下一次遗传预测可能性的总和。在Pod配置中的4个云TPU上进行BERTBASE训练（总共16个TPU芯片）.5在16个CloudTPU（64TPUchipstotal）上进行BERTLARGE训练。每次预训需要4天才能完成。        

**<span style="color:#008B8B;">3.5&emsp;&nbsp;微调程序</span>**       
&emsp;&emsp;对于序列级分类任务，BERT微调很简单。为了获得输入序列的固定维度合并表示，对输入中的第一个标记进行最终隐藏状态（即变换器的输出），其结构对应于特殊的[CLS]字嵌入。我们将该向量表示为C∈RH。在微调期间添加的唯一新参数是分类层W∈RK×H，其中K是分类器标签的数量。标准概率P∈RK用标准softmax，P = softmax（CWT）计算。BERT和W的所有参数都经过联合调整，以最大化正确标签的对数概率。对于跨度级和token级预测任务，必须以任务特定方式稍微修改上述过程。详情见第4节的相应小节。       
&emsp;&emsp;对于微调，大多数模型超参数与预训练相同，但批量大小，学习率和训练时期数除外。辍学概率始终保持在0.1。最佳超参数值是任务特定的，但我们发现以下范围的可能值可以在所有任务中很好地工作：     
&emsp;&emsp;批量大小：16,32       
&emsp;&emsp;学习率：5e-5,3e-5,2e-5     
&emsp;&emsp;时期数量：3,4     
&emsp;&emsp;我们还观察到，大数据集（例如，100k +标记的训练示例）对于较高参数的热带数据集的敏感性要小得多。微调通常非常快，因此简单地对上述参数进行详尽搜索并选择在开发集上表现最佳的模型是合理的。     

**<span style="color:#008B8B;">3.6&emsp;&nbsp;BERT和OpenAI GPT的比较</span>**       
&emsp;&emsp;BERT最具可比性的预训练方法是OpenAI GPT，它在一个大型文本语料库中训练一个左转的Transformer LM。实际上，BERT中的许多设计决策都被有意地选择为尽可能接近GPT，以便可以最小化地比较这两种方法。 这项工作的核心论点是3.3节中提出的两个新的预训练任务占了大多数经验改进，但我们注意到BERT和GPT如何训练之间还存在其他一些差异：     
&emsp;&emsp;GPT接受了BooksCorpus（800M字）的培训; BERT受过BooksCorpus（800M字）和维基百科（2,500M字）的培训。      
&emsp;&emsp;GPT使用句子分隔符（[SEP]）和分类符号（[CLS]），它们仅在微调时引入; BERT在预训练期间学习[SEP]，[CLS]和句子A / B嵌入。     
&emsp;&emsp;GPT接受了1M步骤的培训，批量为32,000字; BERT接受了1Mstepswithabatchsizeof128,000字的培训。      
&emsp;&emsp;GPT对所有的调整实验使用相同的5e-5学习率; BERT选择任务特定的微调学习速率，该速率在开发集上表现最佳。         
&emsp;&emsp;为了解决这些差异的影响，我们在第5.1节中进行了消融实验，证明了这些改进的大部分来自新的预训练任务。      

**<span style="color:#008B8B;">·&emsp;&nbsp;结论</span>**     
&emsp;&emsp;BERT是一个语言表征模型（language representation model），通过超大数据、巨大模型、和极大的计算开销训练而成，在11个自然语言处理的任务中取得了最优（state-of-the-art, SOTA）结果。或许你已经猜到了此模型出自何方，没错，它产自谷歌。估计不少人会调侃这种规模的实验已经基本让一般的实验室和研究员望尘莫及了，但它确实给我们提供了很多宝贵的经验：       
·&emsp;&nbsp;深度学习就是表征学习（Deep learning is representation learning）"We show that pre-trained representations eliminate the needs of many heavily engineered task-specific architectures". 在11项BERT刷出新境界的任务中，大多只在预训练表征（pre-trained representation）微调（fine-tuning）的基础上加一个线性层作为输出（linear output layer）。在序列标注的任务里（e.g. NER），甚至连序列输出的依赖关系都先不管（i.e. non-autoregressive and no CRF），照样秒杀之前的SOTA，可见其表征学习能力之强大。       
·&emsp;&nbsp;训练规模很重要（Scale matters）"One of our core claims is that the deep bidirectionality of BERT, which is enabled by masked LM pre-training, is the single most important improvement of BERT compared to previous work". 这种遮掩（mask）在语言模型上的应用对很多人来说已经不新鲜了，但确是BERT的作者在如此超大规模的数据+模型+算力的基础上验证了其强大的表征学习能力。这样的模型，甚至可以延伸到很多其他的模型，可能之前都被不同的实验室提出和试验过，只是由于规模的局限没能充分挖掘这些模型的潜力，而遗憾地让它们被淹没在了滚滚的paper洪流之中。        
·&emsp;&nbsp;预训练价值很大（Pre-training is important）："We believe that this is the first work to demonstrate that scaling to extreme model sizes also leads to large improvements on very small-scale tasks, provided that the model has been sufficiently pre-trained". 预训练已经被广泛应用在各个领域了（例如：ImageNet for CV, Word2Vec in NLP），多是通过大模型大数据，这样的大模型给小规模任务能带来的提升有几何，作者也给出了自己的答案。BERT模型的预训练是用Transformer做的，但我想换做LSTM或者GRU的话应该不会有太大性能上的差别，当然训练计算时的并行能力就另当别论了。        

